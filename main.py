import time
start_time = time.time()
import os
from dotenv import load_dotenv
import requests
from youtube_transcript_api import YouTubeTranscriptApi
from llama_cpp import Llama, LlamaGrammar
from youtube_metadata import YouTubeMetadataFetcher  # Assuming this class is in youtube_metadata_fetcher.py

# Load environment variables from .env file
load_dotenv()

# Get the API key from environment variables
api_key = os.getenv("YOUTUBE_API_KEY")

# Initialize the YouTubeMetadataFetcher class
fetcher = YouTubeMetadataFetcher(api_key)

# Define the video ID
video_id = "qWbHSOplcvY"

# Retrieve and print metadata using the YouTubeMetadataFetcher class
metadata = fetcher.get_video_metadata(video_id)

print("Local Language Metadata:")
print(f"Title: {metadata['local']['title']}")
print(f"Description: {metadata['local']['description']}")

print("\nEnglish Metadata:")
print(f"Title: {metadata['english']['title']}")
print(f"Description: {metadata['english']['description']}")

# Fetch and print the transcript using the get_transcript_as_string method
transcript_string = fetcher.get_transcript_as_string(video_id)
if transcript_string:
    print("\nTranscript:")
    print(transcript_string)


# Load your GBNF grammar file for Llama
with open('./grammar.gbnf', 'r') as f:
    grammar_text = f.read()

# Create a LlamaGrammar object from the GBNF text
grammar = LlamaGrammar.from_string(grammar_text)

# Load the LLaMA model
llm = Llama.from_pretrained(
    repo_id="ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF",
    filename="meta-llama-3.1-8b-instruct-q4_0.gguf",
    n_ctx=4096
)

# Define the user query
user_query = """
This is autogenerated transcript from audio by a chef, what are the ingredients and instructions?

Based on the provided transcript, extract all the ingredients mentioned.
Provide the answer strictly in valid JSON format like this:
{
    "ingredients": [
        {"name": "ingredient1", "quantity": "amount description"},
        {"name": "ingredient2", "quantity": "amount description"}
        // ... more ingredients
    ],
    "Instructions": "Step by step recipe instructions."
}
"""

# Combine the transcript context with the user's question
prompt_with_context = f"Context: {transcript_string}\n\nUser: {user_query}"

# Generate a completion using LLaMA
response = llm.create_chat_completion(
    messages=[
        {
            "role": "user",
            "content": prompt_with_context
        }
    ],
    grammar=grammar
)

# Print the generated response from LLaMA
print("\nGenerated Recipe Instructions:")
print(response)

# Track and print runtime
end_time = time.time()
print(f"Runtime of the program: {end_time - start_time} seconds")
