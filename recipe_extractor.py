import time
import os
from dotenv import load_dotenv
from llama_cpp import Llama, LlamaGrammar
from youtube_metadata import YouTubeMetadataFetcher  # Assuming this class is in youtube_metadata_fetcher.py

class RecipeExtractor:
    def __init__(self, video_id: str, grammar_file_path: str = './recipe.gbnf'):
        # Load environment variables from .env file
        load_dotenv()

        # Get the API key from environment variables
        self.api_key = os.getenv("YOUTUBE_API_KEY")
        self.video_id = video_id
        self.grammar_file_path = grammar_file_path  # Grammar file path now internal to the class

        # Initialize YouTubeMetadataFetcher
        self.fetcher = YouTubeMetadataFetcher(self.api_key)

        # Load the LLaMA model and the grammar
        self.llm = self.load_llama_model()
        self.grammar = self.load_grammar()

    def load_llama_model(self):
        # Load the LLaMA model
        return Llama.from_pretrained(
            repo_id="ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF",
            filename="meta-llama-3.1-8b-instruct-q4_0.gguf",
            n_ctx=4096
        )

    def load_grammar(self):
        # Load GBNF grammar file
        with open(self.grammar_file_path, 'r') as f:
            grammar_text = f.read()

        # Create and return LlamaGrammar object from GBNF text
        return LlamaGrammar.from_string(grammar_text)

    def fetch_metadata(self):
        # Retrieve video metadata
        metadata = self.fetcher.get_video_metadata(self.video_id)

        print("Local Language Metadata:")
        print(f"Title: {metadata['local']['title']}")
        print(f"Description: {metadata['local']['description']}")

        print("\nEnglish Metadata:")
        print(f"Title: {metadata['english']['title']}")
        print(f"Description: {metadata['english']['description']}")

        return metadata

    def fetch_transcript(self):
        # Fetch transcript using YouTubeMetadataFetcher
        transcript_string = self.fetcher.get_transcript_as_string(self.video_id)
        if transcript_string:
            print("\nTranscript:")
            print(transcript_string)
        return transcript_string

    def generate_recipe(self, transcript_string):
        # Define the user query
        user_query = """
        This is autogenerated transcript from audio by a chef, what are the ingredients and instructions?

        Based on the provided transcript, extract all the ingredients mentioned.
        Provide the answer strictly in valid JSON format like this:
        {
            "ingredients": [
                {"name": "ingredient1", "quantity": "amount description"},
                {"name": "ingredient2", "quantity": "amount description"}
                // ... more ingredients
            ],
            "Instructions": "Step by step recipe instructions."
        }
        """

        # Combine transcript context with user query
        prompt_with_context = f"Context: {transcript_string}\n\nUser: {user_query}"

        # Generate a completion using LLaMA
        response = self.llm.create_chat_completion(
            messages=[
                {
                    "role": "user",
                    "content": prompt_with_context
                }
            ],
            grammar=self.grammar
        )

        # Print the generated recipe
        print("\nGenerated Recipe Instructions:")
        print(response)

        return response

    def run(self):
        # Start time tracking
        start_time = time.time()

        # Fetch metadata
        self.fetch_metadata()

        # Fetch transcript
        transcript_string = self.fetch_transcript()

        if transcript_string:
            # Generate recipe
            self.generate_recipe(transcript_string)

        # End time tracking
        end_time = time.time()
        print(f"Runtime of the program: {end_time - start_time} seconds")


